# -*- coding: utf-8 -*-
"""Salinan dari DEC Update Revisi

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D2dJAT7DMVS5Y8BnTvFvGBxE2fLJLAeS

# IMPORT LIBRARY DAN LOAD DATA
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Mengatur gaya plot untuk visualisasi
plt.style.use("seaborn-v0_8")
sns.set(font_scale=1.2)

# Memuat dataset dari file CSV
df = pd.read_csv("data/katalog_gempa_v2.csv")

# Menampilkan 5 baris pertama dari DataFrame untuk inspeksi awal
df

"""# CEK INFORMASI DATA"""

# Menampilkan informasi umum tentang DataFrame (tipe data, non-null counts, memory usage)
df.info()

# Menampilkan statistik deskriptif untuk kolom numerik
df.describe()

# Menghitung jumlah nilai yang hilang (NaN) untuk setiap kolom
df.isnull().sum()

"""# KONVERSI DATE TIME"""

# Mengonversi kolom 'datetime' ke tipe data datetime
# 'format='mixed'' digunakan untuk menangani format tanggal yang mungkin bervariasi
df['datetime'] = pd.to_datetime(df['datetime'], format='mixed')

# Mengurutkan DataFrame berdasarkan kolom 'datetime'
df = df.sort_values('datetime')

# Mereset indeks DataFrame setelah pengurutan dan menghapus indeks lama
df = df.reset_index(drop=True)

# Menampilkan 5 baris pertama dari DataFrame setelah konversi dan pengurutan
df

"""# EDA (Exploratory Data Analysis)

### 1. Distribusi Magnitudo
"""

# Membuat figure untuk plot dengan ukuran tertentu
plt.figure(figsize=(10,5))

# Membuat histogram dari kolom 'magnitude' dengan 50 bins dan estimasi densitas kernel (kde)
sns.histplot(df['magnitude'], bins=50, kde=True)

# Memberikan judul pada plot
plt.title("Distribusi Magnitudo Gempa")

# Memberikan label pada sumbu x
plt.xlabel("Magnitudo")

# Memberikan label pada sumbu y
plt.ylabel("Frekuensi")

# Menampilkan plot
plt.show()

"""### 2. Distribusi Kedalaman Gempa"""

# Membuat figure untuk plot dengan ukuran tertentu
plt.figure(figsize=(10,5))

# Membuat histogram dari kolom 'depth' dengan 50 bins, estimasi densitas kernel (kde), dan warna orange
sns.histplot(df['depth'], bins=50, kde=True, color='orange')

# Memberikan judul pada plot
plt.title("Distribusi Kedalaman Gempa")

# Memberikan label pada sumbu x
plt.xlabel("Depth (km)")

# Memberikan label pada sumbu y
plt.ylabel("Frekuensi")

# Menampilkan plot
plt.show()

"""### 3. Scatter Lokasi Gempa"""

# Membuat figure untuk plot dengan ukuran tertentu
plt.figure(figsize=(10,6))

# Membuat scatter plot dari longitude dan latitude untuk menunjukkan sebaran lokasi gempa
# s=2 mengatur ukuran marker, alpha=0.5 mengatur transparansi
plt.scatter(df['longitude'], df['latitude'], s=2, alpha=0.5)

# Memberikan judul pada plot
plt.title("Sebaran Lokasi Gempa")

# Memberikan label pada sumbu x
plt.xlabel("Longitude")

# Memberikan label pada sumbu y
plt.ylabel("Latitude")

# Menampilkan plot
plt.show()

"""### 4. Magnitudo terhadap Waktu"""

# Membuat figure untuk plot dengan ukuran tertentu
plt.figure(figsize=(15,5))

# Membuat plot garis untuk melihat perubahan magnitudo gempa terhadap waktu
# linewidth=0.6 mengatur ketebalan garis
plt.plot(df['datetime'], df['magnitude'], linewidth=0.6)

# Memberikan judul pada plot
plt.title("Magnitudo Gempa terhadap Waktu")

# Memberikan label pada sumbu x
plt.xlabel("Tahun")

# Memberikan label pada sumbu y
plt.ylabel("Magnitudo")

# Menampilkan plot
plt.show()

"""### 5. Depth vs Magnitude Scatter"""

# Membuat figure untuk plot dengan ukuran tertentu
plt.figure(figsize=(10,6))

# Membuat scatter plot menggunakan seaborn untuk melihat korelasi antara kedalaman dan magnitudo
# data=df menentukan DataFrame yang digunakan, x dan y menentukan kolom untuk sumbu, alpha=0.3 mengatur transparansi
sns.scatterplot(data=df, x="depth", y="magnitude", alpha=0.3)

# Memberikan judul pada plot
plt.title("Korelasi Kedalaman vs Magnitudo")

# Menampilkan plot
plt.show()

"""#  PREPROCESSING

### Ambil Kolom yang Dibutuhkan
"""

# --- 1. PREPROCESSING DASAR ---
# Membuat kolom 'year' dari datetime
df['year'] = df['datetime'].dt.year

# --- 2. DATA UNTUK CLUSTERING ---
# Memilih kolom relevan untuk clustering
df_clustering = df[['year', 'magnitude', 'depth', 'latitude', 'longitude']].copy()

# Interpolasi untuk nilai hilang pada magnitude & depth
df_clustering['magnitude'] = df_clustering['magnitude'].interpolate()
df_clustering['depth'] = df_clustering['depth'].interpolate()

print("Informasi df_clustering setelah preprocessing:")
df_clustering.info()
print(df_clustering.head())


# --- 3. DATA UNTUK MODEL DNN & LSTM ---
# Memilih kolom fitur umum untuk DNN dan LSTM
df_common_features = df[['magnitude', 'depth', 'latitude', 'longitude']].copy()

# Interpolasi nilai hilang
df_common_features = df_common_features.interpolate()

# Menghapus data duplikat
df_common_features.drop_duplicates(inplace=True)

# Simpan hasil preprocessing
preprocessed_file_path = "preprocessed_earthquake_data.csv"
df_common_features.to_csv(preprocessed_file_path, index=False)

print(f"\nData preprocessed disimpan ke: {preprocessed_file_path}")
print(df_common_features.head())

"""### Normalisasi (mengubah nilai data ke bentuk float)"""

from sklearn.preprocessing import MinMaxScaler
import pandas as pd
import joblib # Untuk menyimpan scaler

# Memuat data yang sudah dipreproses dari file
df_lstm_input = pd.read_csv("preprocessed_earthquake_data.csv")

# Menginisialisasi MinMaxScaler untuk menormalisasi data
# Normalisasi akan menskala fitur ke rentang 0-1
scaler_lstm = MinMaxScaler()

# Melakukan fit dan transform pada data untuk menskalakannya
scaled_for_lstm = scaler_lstm.fit_transform(df_lstm_input)

# Menyimpan scaler LSTM
joblib.dump(scaler_lstm, 'scaler_lstm.pkl')
print("scaler_lstm disimpan ke scaler_lstm.pkl")

# Menampilkan array data yang sudah diskala
scaled_for_lstm

"""PERSIAPAN MODELING DEC <br>
<br>

Menyiapkan data buat modeling Deep Embedded Clustering (DEC) dan melakukan preprocessing spesifik untuk DEC, yaitu standardisasi data, dan juga menyimpan scaler yang digunakan.
"""

from sklearn.preprocessing import StandardScaler

# Memuat data yang sudah dipreproses dari file
df_dec_input = pd.read_csv("preprocessed_earthquake_data.csv")

# Menginisialisasi StandardScaler untuk menstandardisasi data (mean=0, std=1)
scaler_dec = StandardScaler()

# Melakukan fit dan transform pada data untuk menskalakannya
X_dec = scaler_dec.fit_transform(df_dec_input)

# Menyimpan scaler DEC
joblib.dump(scaler_dec, 'scaler_dec.pkl')
print("scaler_dec disimpan ke scaler_dec.pkl")

# Menampilkan bentuk (shape) dari data yang sudah diskala
X_dec.shape

"""MODEL AUTOENCODER"""

from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model

# Menentukan dimensi input, yaitu jumlah fitur dalam data
input_dim = X_dec.shape[1]

# Input layer dari autoencoder
input_layer = Input(shape=(input_dim,))

# ENCODER
# Layer Dense pertama pada encoder dengan 64 unit dan aktivasi ReLU
encoder = Dense(64, activation='relu')(input_layer)
# Layer Dense kedua pada encoder dengan 32 unit dan aktivasi ReLU
encoder = Dense(32, activation='relu')(encoder)
# Latent space, layer Dense dengan 10 unit dan aktivasi ReLU
latent = Dense(10, activation='relu')(encoder)

# DECODER
# Layer Dense pertama pada decoder dengan 32 unit dan aktivasi ReLU
decoder = Dense(32, activation='relu')(latent)
# Layer Dense kedua pada decoder dengan 64 unit dan aktivasi ReLU
decoder = Dense(64, activation='relu')(decoder)
# Output layer Dense yang mengembalikan dimensi input asli
output_layer = Dense(input_dim)(decoder)

# Membuat model autoencoder lengkap (encoder + decoder)
autoencoder = Model(inputs=input_layer, outputs=output_layer)

# Mengkompilasi autoencoder
# optimizer='adam' dan loss='mse' karena ini adalah tugas rekonstruksi data (regresi)
autoencoder.compile(optimizer='adam', loss='mse')

# Menampilkan ringkasan arsitektur model autoencoder
autoencoder.summary()

"""TRAINING AUTOENCODER"""

# Melatih model autoencoder
history_ae = autoencoder.fit(
    X_dec, X_dec, # Input dan target adalah data yang sama untuk autoencoder
    epochs=30, # Jumlah epoch pelatihan
    batch_size=256, # Ukuran batch
    validation_split=0.1 # Menggunakan 10% data untuk validasi
)

# Menyimpan model autoencoder yang telah dilatih
autoencoder.save('model/autoencoder_model.h5')
print("Model Autoencoder disimpan ke autoencoder_model.h5")

# Ambil data loss
loss = history_ae.history['loss']
val_loss = history_ae.history['val_loss']
epochs = range(1, len(loss) + 1)

plt.figure(figsize=(8,5))

plt.plot(epochs, loss, label='Training Loss')
plt.plot(epochs, val_loss, label='Validation Loss')

plt.xlabel('Epoch')
plt.ylabel('Loss (MSE)')
plt.title('Training vs Validation Loss (Log Scale)')
plt.legend()
plt.grid(True)

# Gunakan sumbu Y logaritmik
plt.yscale('log')

plt.show()

"""AMBIL LATENT SPACE"""

# Membuat model encoder terpisah untuk mengekstrak representasi latent space
# Input model adalah input_layer, dan outputnya adalah layer 'latent'
encoder_model = Model(inputs=input_layer, outputs=latent)

# Melakukan prediksi (encoding) pada data yang sudah diskala (X_dec) untuk mendapatkan latent space
Z = encoder_model.predict(X_dec)

# Menyimpan model encoder
encoder_model.save('encoder_model.h5')
print("Model Encoder disimpan ke encoder_model.h5")

# Menampilkan bentuk (shape) dari representasi latent space
Z.shape

from sklearn.cluster import KMeans

# Membuat DataFrame baru df_Z berisi latent space Z dengan nama kolom:
# latent_feature_0, latent_feature_1, ..., latent_feature_n
latent_col_names = [f'latent_feature_{i}' for i in range(Z.shape[1])]
df_Z = pd.DataFrame(Z, columns=latent_col_names)

# Menggabungkan kolom year, latitude, dan longitude dari df_clustering
# dengan latent features Z menjadi satu DataFrame df_latent_features
df_latent_features = pd.concat(
    [df_clustering[['year', 'latitude', 'longitude']].reset_index(drop=True), df_Z],
    axis=1
)

# Membuat dictionary kosong untuk menyimpan hasil clustering per tahun
df_clustered_by_year_dec = {}

# Mengambil daftar tahun unik dari DataFrame
years_dec = df_latent_features['year'].unique()

# Melakukan proses clustering untuk setiap tahun
for year in years_dec:

    # Memfilter data untuk setiap tahun menjadi DataFrame df_year_latent
    df_year_latent = df_latent_features[df_latent_features['year'] == year].copy()

    # Mengambil hanya kolom latent feature untuk proses clustering
    features_for_clustering = df_year_latent[latent_col_names]

    # Membuat model K-Means dengan jumlah cluster = 2
    kmeans_dec = KMeans(n_clusters=2, random_state=42, n_init='auto')

    # Melakukan training model dan memprediksi label cluster
    cluster_labels_dec = kmeans_dec.fit_predict(features_for_clustering)

    # Menambahkan kolom 'cluster_label' ke df_year_latent
    df_year_latent['cluster_label'] = cluster_labels_dec

    # Menyimpan DataFrame hasil clustering ke dictionary berdasarkan tahun
    df_clustered_by_year_dec[year] = df_year_latent

# Menampilkan informasi hasil
print("Proses K-Means berdasarkan latent space DEC selesai untuk semua tahun.")
print("5 baris pertama dari DataFrame hasil clustering (tahun pertama):")

if years_dec.size > 0:
    print(df_clustered_by_year_dec[years_dec[0]].head())
else:
    print("Tidak ditemukan tahun untuk proses clustering.")

# Menggabungkan seluruh DataFrame hasil clustering per tahun
# yang tersimpan di dalam dictionary df_clustered_by_year_dec
# menjadi satu DataFrame besar. Setiap DataFrame tahun digabung
# secara vertikal (baris demi baris).
df_clustered_all_dec = pd.concat(df_clustered_by_year_dec.values(), ignore_index=True)

# Menampilkan informasi bahwa DataFrame gabungan telah dibuat
print("DataFrame gabungan df_clustered_all_dec berhasil dibuat. 5 baris pertama:")

# Menampilkan DataFrame gabungan
df_clustered_all_dec

# !pip install cartopy

import cartopy.crs as ccrs
import cartopy.feature as cfeature
import matplotlib.patches as mpatches

# Loop semua tahun
for year, yearly_df_dec in df_clustered_by_year_dec.items():
    plt.figure(figsize=(14, 12))
    ax = plt.axes(projection=ccrs.PlateCarree())

    # Fokus Indonesia
    ax.set_extent([90, 145, -15, 10], crs=ccrs.PlateCarree())

    # Fitur geografis
    ax.add_feature(cfeature.LAND, edgecolor='black', zorder=1)
    ax.add_feature(cfeature.OCEAN, zorder=0)
    ax.add_feature(cfeature.COASTLINE, zorder=2)
    ax.add_feature(cfeature.BORDERS, linestyle=':', zorder=2)

    # HEATMAP DENSITY PER CLUSTER
    legend_handles = []
    legend_labels = []

    for cluster_label in sorted(yearly_df_dec["cluster_label"].unique()):
        # Data per cluster
        cluster_data = yearly_df_dec[yearly_df_dec["cluster_label"] == cluster_label]

        # Warna cluster
        color = "blue" if cluster_label == 0 else "red"
        label = f"Cluster {cluster_label}"

        # Plot KDE density heatmap
        sns.kdeplot(
            x=cluster_data["longitude"],
            y=cluster_data["latitude"],
            fill=True,
            levels=15,
            alpha=0.45,
            color=color,
            ax=ax
        )

        # Legend untuk KDE
        legend_handles.append(mpatches.Patch(color=color, alpha=0.45))
        legend_labels.append(label)

    # SCATTER POINTS CLUSTER
    sns.scatterplot(
        data=yearly_df_dec,
        x="longitude",
        y="latitude",
        hue="cluster_label",
        palette={0: "blue", 1: "red"},
        s=22,
        alpha=0.8,
        ax=ax,
        zorder=3
    )

    # Judul & Label
    plt.title(f"Sebaran Titik & Heatmap Density Gempa (DEC) - Tahun {year}", fontsize=14)
    plt.xlabel("Longitude")
    plt.ylabel("Latitude")

    # Gridlines
    ax.gridlines(draw_labels=True, dms=True, x_inline=False, y_inline=False)

    # Legend
    ax.legend(handles=legend_handles, labels=legend_labels, title="Cluster", loc="upper right")

    plt.show()

for year, yearly_df_dec in df_clustered_by_year_dec.items():
    # Menghitung jumlah kemunculan setiap label cluster pada tahun tersebut
    cluster_counts = yearly_df_dec['cluster_label'].value_counts()

    if not cluster_counts.empty:
        # Menentukan cluster yang paling dominan (jumlah kemunculannya paling banyak)
        dominant_cluster = cluster_counts.idxmax()
        dominant_count = cluster_counts.max()
        print(f"Tahun {year}: Cluster Dominan = {dominant_cluster} (Jumlah Kemunculan = {dominant_count})")
    else:
        # Jika tidak ada data cluster pada tahun tersebut
        print(f"Tahun {year}: Tidak ditemukan data cluster.")

from sklearn.metrics import silhouette_score

yearly_silhouette_scores_dec = {}

# Melakukan iterasi untuk setiap tahun pada dictionary df_clustered_by_year_dec
for year, yearly_df_dec in df_clustered_by_year_dec.items():

    # Mengambil daftar kolom latent feature untuk tahun tersebut
    latent_col_names = [col for col in yearly_df_dec.columns if 'latent_feature_' in col]
    features_for_silhouette = yearly_df_dec[latent_col_names]

    # Mengambil label cluster untuk tahun tersebut
    cluster_labels = yearly_df_dec['cluster_label']

    # Silhouette Score hanya dapat dihitung jika jumlah cluster > 1 dan jumlah sampel > 1
    if len(np.unique(cluster_labels)) > 1 and len(cluster_labels) > 1:

        # Menghitung Silhouette Score
        silhouette_avg = silhouette_score(features_for_silhouette, cluster_labels)
        yearly_silhouette_scores_dec[year] = silhouette_avg
        print(f"Silhouette Score untuk tahun {year}: {silhouette_avg:.4f}")

    else:
        # Jika cluster hanya satu atau jumlah data terlalu sedikit
        yearly_silhouette_scores_dec[year] = None
        print(f"Silhouette Score untuk tahun {year} tidak dapat dihitung: "
              f"Cluster atau jumlah sampel tidak mencukupi.")

# Menampilkan seluruh nilai Silhouette Score untuk setiap tahun
print("\nSilhouette Score per Tahun (DEC):")
print(yearly_silhouette_scores_dec)

"""# DETEKSI ANOMALI

### Hitung Reconstruction Error

### untuk mengukur seberapa baik Autoencoder dapat merekonstruksi setiap titik data
"""

# Rekonstruksi data
X_dec_pred = autoencoder.predict(X_dec)

# Hitung error MSE per sampel
reconstruction_error = np.mean(np.square(X_dec - X_dec_pred), axis=1)

"""### Menetapkan threshold otomatis
### Menggunakan mean + 3*std
Menetapkan threshold secara otomatis menggunakan rumus mean + 3 * std (rata-rata ditambah tiga kali standar deviasi)
"""

threshold = np.mean(reconstruction_error) + 3 * np.std(reconstruction_error)
print("Threshold anomaly:", threshold)

"""### Tentukan mana yang anomali"""

anomaly_flags = reconstruction_error > threshold

"""### Gabungkan hasil dengan data asli"""

df_results = df_dec_input.copy()
df_results['reconstruction_error'] = reconstruction_error
df_results['is_anomaly'] = anomaly_flags.astype(int)  # 1 = anomali, 0 = normal

df_results

# Data normal (label 0)
df_normal = df_results[df_results['is_anomaly'] == 0]
print("=== DATA NORMAL (label 0) ===")
df_normal

# Data anomali (label 1)
df_anomaly = df_results[df_results['is_anomaly'] == 1]
print("\n=== DATA ANOMALI (label 1) ===")
df_anomaly

print("Jumlah data normal :", len(df_normal))
print("Jumlah data anomali:", len(df_anomaly))

# Top 10 anomali dengan error terbesar
df_top_anomaly = df_results.sort_values(by='reconstruction_error', ascending=False).head(10)
df_top_anomaly